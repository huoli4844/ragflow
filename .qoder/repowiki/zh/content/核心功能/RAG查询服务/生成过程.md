# 生成过程

<cite>
**本文档引用的文件**
- [naive.py](file://rag/app/naive.py)
- [qa.py](file://rag/app/qa.py)
- [chat_model.py](file://rag/llm/chat_model.py)
- [conversation_service.py](file://api/db/services/conversation_service.py)
- [dialog_service.py](file://api/db/services/dialog_service.py)
- [generator.py](file://rag/prompts/generator.py)
</cite>

## 目录
1. [引言](#引言)
2. [生成环节概述](#生成环节概述)
3. [NaiveRAGService 与 QARAGService](#naiveragservice-与-qaragservice)
4. [LLM 调用封装](#llm-调用封装)
5. [会话状态管理](#会话状态管理)
6. [流式响应、超时与错误重试](#流式响应超时与错误重试)
7. [提示词工程最佳实践](#提示词工程最佳实践)
8. [结论](#结论)

## 引言
本文档详细阐述了RAG（检索增强生成）查询中的生成环节。重点分析了`NaiveRAGService`和`QARAGService`如何将检索到的上下文片段与用户问题结合，构造提示词（prompt）并调用大语言模型（LLM）进行回答生成。同时，文档深入探讨了`chat_model.py`中LLM调用的封装逻辑，以及`conversation_service.py`如何管理会话状态和历史消息。此外，还说明了系统如何处理流式响应、超时和错误重试，并讨论了提示词工程的最佳实践，如上下文压缩、指令微调和防止幻觉（hallucination）的策略。

## 生成环节概述
在RAG系统中，生成环节是将检索阶段获得的相关上下文信息与用户原始问题相结合，通过精心设计的提示词（prompt）引导大语言模型（LLM）生成准确、相关且信息丰富的回答的过程。该环节是整个RAG流程的核心，直接决定了最终输出的质量。系统通过`NaiveRAGService`和`QARAGService`两种服务来处理不同类型的文档和问答对，利用`LLMBundle`封装的LLM调用接口，并通过`conversation_service`和`dialog_service`管理会话的完整生命周期。

## NaiveRAGService 与 QARAGService

`NaiveRAGService`和`QARAGService`是RAG系统中负责处理不同数据源并生成回答的核心服务。它们的主要区别在于处理的文档类型和解析逻辑。

`NaiveRAGService`（在代码中通过`rag/app/naive.py`中的`chunk`函数实现）采用一种通用的“朴素”方法来处理各种文档格式，如PDF、DOCX、TXT和Markdown。它首先使用相应的解析器（如`Pdf`、`Docx`）将文档解析成文本段落和表格，然后根据配置的分隔符（delimiter）和最大token数进行分块。对于检索到的上下文，它会将这些文本块与用户问题直接拼接，形成最终的提示词。其核心逻辑是将文档内容视为连续的文本流进行处理。

`QARAGService`（在代码中通过`rag/app/qa.py`中的`chunk`函数实现）则专门用于处理结构化的问答对（Q&A pairs）。它支持Excel、CSV、TXT、PDF和Markdown等格式的文件，其目标是直接从文件中提取出成对的问题和答案。例如，对于Excel文件，它会读取前两列作为问题和答案；对于Markdown文件，它会识别以`#`开头的标题作为问题，并将后续内容作为答案。这种方式生成的上下文本身就是高质量的问答对，因此在生成回答时，可以直接利用这些结构化的信息。

两种服务在生成提示词时都遵循类似的模式：将检索到的上下文片段（无论是文本块还是问答对）与用户问题结合，并添加必要的系统指令。最终的提示词会传递给LLM进行处理。

**Section sources**
- [naive.py](file://rag/app/naive.py#L618-L706)
- [qa.py](file://rag/app/qa.py#L313-L462)

## LLM 调用封装

LLM的调用逻辑在`rag/llm/chat_model.py`文件中被高度封装，主要通过`Base`类及其子类（如`GptTurbo`、`AzureChat`等）来实现。核心的调用接口是`async_chat_streamly`方法，它支持异步和流式响应。

`LLMBundle`类（在`api/db/services/llm_service.py`中定义）是调用LLM的主要入口。它根据租户（tenant）ID、模型类型（LLMType）和模型名称来初始化一个具体的LLM实例（如`chat_mdl`）。当需要生成回答时，`dialog_service`会调用`chat_mdl.async_chat_streamly`方法。

该方法的参数包括：
- `system`: 系统提示词，定义了LLM的角色和行为准则。
- `history`: 对话历史，一个包含`role`（角色）和`content`（内容）的字典列表。
- `gen_conf`: 生成配置，包含`temperature`、`max_tokens`等参数。

在`async_chat_streamly`方法内部，系统会处理重试逻辑、超时和错误分类。例如，当遇到速率限制（RATE_LIMIT）或服务器错误（SERVER_ERROR）时，系统会根据配置进行指数退避重试。同时，该方法会清理和验证生成配置，确保只传递LLM支持的参数。

**Section sources**
- [chat_model.py](file://rag/llm/chat_model.py#L139-L197)
- [llm_service.py](file://api/db/services/llm_service.py#L386-L463)

## 会话状态管理

会话状态的管理由`conversation_service.py`和`dialog_service.py`两个模块协同完成。

`conversation_service.py`负责持久化存储会话数据。每个会话（`Conversation`）记录了与特定对话相关的所有信息，包括：
- `id`: 会话的唯一标识符。
- `dialog_id`: 关联的对话（`Dialog`）ID。
- `message`: 消息列表，存储了完整的对话历史。
- `reference`: 引用信息，记录了生成回答时所依据的上下文片段。

`dialog_service.py`则负责会话的运行时逻辑。`async_chat`函数是生成回答的核心入口。它首先根据`dialog`对象的配置获取所需的模型（如`chat_mdl`），然后构建消息历史。在构建消息时，它会过滤掉系统角色的消息，并将用户上传的文件附件内容附加到最新的用户消息中。对于流式响应，它会逐字节地接收LLM的输出，并在达到一定token数量时进行流式传输，以优化用户体验。

当用户发起一个新的查询时，系统会创建一个新的消息对象并将其添加到会话的`message`列表中，然后调用`async_chat`进行处理。生成的回答同样会被添加到消息列表中，从而保持了完整的对话上下文。

**Section sources**
- [conversation_service.py](file://api/db/services/conversation_service.py#L92-L167)
- [dialog_service.py](file://api/db/services/dialog_service.py#L182-L217)

## 流式响应、超时与错误重试

系统通过异步和流式处理来支持实时的、低延迟的响应。

**流式响应**：通过`async_chat_streamly`方法，LLM的输出可以被逐字（或逐token）地发送给客户端。这使得用户可以在答案生成的同时就开始阅读，极大地提升了交互体验。在`dialog_service.py`中，`async_chat_solo`函数通过`async for`循环接收流式输出，并在累积到一定量的文本（默认16个token）后立即发送。

**超时**：系统在多个层面设置了超时机制。在`chat_model.py`的`Base`类初始化时，会设置一个全局的`timeout`（默认600秒）。此外，Quart应用服务器也配置了`RESPONSE_TIMEOUT`和`BODY_TIMEOUT`，以防止长时间运行的请求阻塞服务器。

**错误重试**：系统实现了健壮的错误重试机制。在`chat_model.py`中，`_exceptions_async`方法会根据错误类型（通过`_classify_error`方法识别）决定是否进行重试。对于可重试的错误（如`ERROR_RATE_LIMIT`、`ERROR_SERVER`），系统会进行指数退避重试，最多尝试`max_retries`次（默认5次）。这确保了在面对临时性网络或服务故障时，系统能够自动恢复。

**Section sources**
- [chat_model.py](file://rag/llm/chat_model.py#L230-L244)
- [dialog_service.py](file://api/db/services/dialog_service.py#L198-L211)

## 提示词工程最佳实践

系统通过多种策略来优化提示词，以提高生成质量并减少幻觉。

**上下文压缩**：虽然代码中没有显式的压缩算法，但通过`parser_config`中的`chunk_token_num`参数，系统在分块阶段就控制了上下文的长度，避免了过长的输入。此外，`async_chat_streamly`方法会在LLM因长度限制而截断输出时，自动添加一个通知（如“...\nThe answer is truncated...”），向用户透明地说明情况。

**指令微调**：系统通过`prompt_config`中的`system`字段来提供详细的系统指令。例如，在`rag/prompts/generator.py`中定义的`PROMPTS["naive_rag_response"]`模板，明确指示LLM“基于文档片段生成简洁的回答”，并“不要包含文档片段中未提供的信息”。这种清晰的指令有助于引导LLM的行为。

**防止幻觉**：防止幻觉是RAG的核心优势。系统通过以下方式实现：
1.  **基于证据的回答**：LLM被明确要求只根据提供的上下文（Document Chunks）生成回答。
2.  **引用机制**：生成的回答会关联到具体的上下文片段（chunks），用户可以追溯信息来源。
3.  **过滤不良引用**：系统定义了`BAD_CITATION_PATTERNS`，用于识别和修复不规范的引用格式，确保引用的准确性。

**Section sources**
- [generator.py](file://rag/prompts/generator.py#L298-L310)
- [dialog_service.py](file://api/db/services/dialog_service.py#L244-L249)

## 结论
RAG系统的生成环节是一个复杂而精密的流程，涉及服务选择、提示词构造、LLM调用、会话管理和错误处理等多个方面。`NaiveRAGService`和`QARAGService`为不同类型的输入数据提供了灵活的处理方案。通过`chat_model.py`的封装，系统实现了对多种LLM后端的统一调用，并具备了处理流式响应、超时和错误重试的能力。`conversation_service`和`dialog_service`共同确保了会话状态的完整性和一致性。最后，通过精心设计的提示词工程，系统能够有效地引导LLM生成基于证据的、高质量的回答，最大限度地减少了幻觉现象的发生。这一系列机制共同构成了一个强大、可靠且用户友好的RAG生成引擎。